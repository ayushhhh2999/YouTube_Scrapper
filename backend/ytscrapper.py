from youtube_transcript_api import YouTubeTranscriptApi
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
import re
from langchain_core.documents import Document


def extract_video_id(youtube_url: str) -> str:
    """Extract video ID from YouTube URL"""
    pattern = r"(?:v=|\/)([0-9A-Za-z_-]{11}).*"
    match = re.search(pattern, youtube_url)
    if not match:
        raise ValueError("Invalid YouTube URL")
    return match.group(1)


def scrapper(url: str) -> str:
    video_id = extract_video_id(url)
    #"""Fetch transcript text from YouTube"""
    #print(f"Attributes of YouTubeTranscriptApi: {dir(YouTubeTranscriptApi)}") # Debugging line
    # retrieve the available transcripts
    ytt_api = YouTubeTranscriptApi()
    transcript_list = ytt_api.list(video_id)
    codes = []
    res = ""
# iterate over all available transcripts
    for transcript in transcript_list:

    # the Transcript object provides metadata properties
        codes.append(transcript.language_code)
    '''print(
        transcript.video_id,
        transcript.language,
        transcript.language_code,
        # whether it has been manually created or generated by YouTube
        transcript.is_generated,
        # whether this transcript can be translated or not
        transcript.is_translatable,
        # a list of languages the transcript can be translated to
        transcript.translation_languages,
    )'''
    if "en-US" in codes:
        result = ytt_api.fetch(video_id,languages=['en-US'])
        res = " ".join([item.text for item in result])
    else:
        result = ytt_api.fetch(video_id,languages=[codes[-1]])
        res = " ".join([item.text for item in result])
    print(" ".join([item.text for item in result]))
    print(res)
    return chunk(res)


def chunk(transcript: str):
    """Convert transcript into embeddings and store in FAISS"""

    # 1. Split text into chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )
    chunks = splitter.split_text(transcript)
    return [
        Document(
            page_content=chunk,
            metadata={"source": "youtube"}
        )
        for chunk in chunks
    ]
    # 2. Load embedding mode
    # 3. Create FAISS vector store
    '''vector_store = FAISS.from_texts(chunks, embedding=embeddings)

    # 4. Save locally
    vector_store.save_local("youtube_vector_db")

    print(f"Stored {len(chunks)} chunks into vector database")'''


